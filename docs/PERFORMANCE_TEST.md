```markdown
# ğŸ“Š æ€§èƒ½æµ‹è¯•æŒ‡å—

> æœ¬é¡¹ç›®å·²æ„å»ºé«˜å¹¶å‘ã€ä½å»¶è¿Ÿçš„å¤šæ¨¡å‹æ¨ç†å¹³å°ã€‚æœ¬æŒ‡å—æä¾›æ ‡å‡†åŒ–çš„æ€§èƒ½æµ‹è¯•æ–¹æ³•ï¼Œç”¨äºè¯„ä¼°ç³»ç»Ÿåœ¨ä¸åŒè´Ÿè½½ä¸‹çš„ **QPSã€å»¶è¿Ÿã€æ˜¾å­˜å ç”¨ä¸ç¨³å®šæ€§**ï¼Œå¹¶éªŒè¯ç›‘æ§ç³»ç»Ÿçš„å‡†ç¡®æ€§ã€‚

---

## ğŸ¯ æµ‹è¯•ç›®æ ‡

| æŒ‡æ ‡ | ç›®æ ‡å€¼ | è¯´æ˜ |
|------|--------|------|
| **å•æ¨¡å‹æ˜¾å­˜å ç”¨** | < 7.5 GB | åŒæ¨¡å‹å¯å…±å­˜äº 12GB GPU |
| **P99 å»¶è¿Ÿ** | < 1.5s | max_new_tokens=128ï¼Œ99% è¯·æ±‚æ»¡è¶³ |
| **QPSï¼ˆåŒæ¨¡å‹æ··åˆï¼‰** | â‰¥ 10 | å¹¶å‘ 50+ åœºæ™¯ä¸‹ |
| **å†·å¯åŠ¨æ—¶é—´** | ç›¸æ¯” FP16 ç¼©çŸ­ 60% | æ¨¡å‹åŠ è½½æ—¶é—´ä¼˜åŒ– |
| **é”™è¯¯ç‡** | < 1% | è¶…æ—¶æˆ–æœåŠ¡ä¸å¯è¾¾ |

---

## ğŸ§° æµ‹è¯•ç¯å¢ƒ

| é¡¹ç›® | é…ç½® |
|------|------|
| ç³»ç»Ÿ | WSL2 (Ubuntu 22.04) |
| GPU | NVIDIA RTX 4070 Super (12GB) |
| é©±åŠ¨ | NVIDIA Driver 551.86 |
| CUDA | 12.4 |
| Docker | 24.0.7 |
| TGI é•œåƒ | `ghcr.io/huggingface/text-generation-inference:latest` |
| æ¨¡å‹ | `Qwen2-7B-Instruct-GPTQ-Int4`, `Meta-Llama-3.1-8B-Instruct-GPTQ-INT4` |

> âœ… æ‰€æœ‰æœåŠ¡é€šè¿‡ `docker-compose up` å¯åŠ¨ï¼Œç½‘å…³è¿è¡Œäº `http://localhost:8000`

---

## ğŸ§ª æµ‹è¯•æ–¹æ³•ä¸€ï¼šä½¿ç”¨ Locust è¿›è¡Œå‹åŠ›æµ‹è¯•ï¼ˆæ¨èï¼‰

### 1. å®‰è£… Locust

```bash
pip install locust
```

### 2. åˆ›å»ºæµ‹è¯•è„šæœ¬ `scripts/load_test.py`

```python
# scripts/load_test.py
import json
from locust import HttpUser, task, between
import random

class LLMUser(HttpUser):
    wait_time = between(1, 3)  # ç”¨æˆ·é—´éš” 1-3 ç§’

    # æ¨¡æ‹Ÿä¸åŒåœºæ™¯çš„è¾“å…¥
    prompts = {
        "qa": "å…‰åˆä½œç”¨çš„åŸç†æ˜¯ä»€ä¹ˆï¼Ÿ",
        "chat": "è¯·ç”¨ä¸­æ–‡ä»‹ç»ä½ è‡ªå·±ã€‚",
        "summarize": "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œæ—¨åœ¨åˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿâ€¦â€¦",
        "rewrite": "è¿™ä¸ªäº§å“éå¸¸å¥½ï¼Œå¤§å®¶éƒ½å¾ˆå–œæ¬¢ã€‚"
    }

    @task(4)
    def generate_qwen2(self):
        self._send_request("qwen2")

    @task(4)
    def generate_llama3(self):
        self._send_request("llama3")

    def _send_request(self, model_id):
        scenario = random.choice(["qa", "chat", "summarize", "rewrite"])
        prompt = self.prompts[scenario]

        payload = {
            "model_id": model_id,
            "inputs": prompt,
            "scenario": scenario,
            "parameters": {
                "max_new_tokens": random.choice([64, 128]),
                "temperature": random.choice([0.7, 0.9]),
                "top_p": 0.9
            }
        }

        with self.client.post("/v1/generate", json=payload, catch_response=True) as resp:
            if resp.status_code != 200:
                resp.failure(f"HTTP {resp.status_code}: {resp.text}")
```

### 3. å¯åŠ¨ Locust æµ‹è¯•

```bash
cd scripts
locust -f load_test.py --host http://localhost:8000
```

### 4. é…ç½®å¹¶å‘å‚æ•°

è®¿é—® [http://localhost:8089](http://localhost:8089)ï¼Œè®¾ç½®ï¼š
- **Number of users**: `50`
- **Spawn rate**: `5` users/sec

### 5. è¿è¡Œå¹¶è§‚å¯Ÿç»“æœ

- æŸ¥çœ‹ **QPS**ã€**å¹³å‡/ä¸­ä½/95%/99% å»¶è¿Ÿ**
- è§‚å¯Ÿé”™è¯¯ç‡ï¼ˆåº” < 1%ï¼‰
- åœ¨ Grafana ä¸­æŸ¥çœ‹ `requests.total` å’Œ `request.latency`

---

## ğŸ§ª æµ‹è¯•æ–¹æ³•äºŒï¼šä½¿ç”¨ Python è„šæœ¬è¿›è¡Œååæµ‹è¯•

### 1. åˆ›å»º `scripts/stress_test.py`

```python
# scripts/stress_test.py
import asyncio
import aiohttp
import time
import json
from typing import List

URL = "http://localhost:8000/v1/generate"
PAYLOADS = [
    {
        "model_id": "qwen2",
        "inputs": "è¯·å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—",
        "scenario": "qa",
        "parameters": {"max_new_tokens": 128}
    },
    {
        "model_id": "llama3",
        "inputs": "Explain quantum mechanics in simple terms",
        "scenario": "qa",
        "parameters": {"max_new_tokens": 128}
    }
]

async def send_request(session, payload, idx):
    start = time.time()
    try:
        async with session.post(URL, json=payload) as resp:
            result = await resp.json()
            latency = time.time() - start
            print(f"âœ… Req-{idx:2d} | {payload['model_id']:6s} | "
                  f"tokens={len(result.get('generated_text', '').split()):3d} | "
                  f"latency={latency:.2f}s")
            return latency, True
    except Exception as e:
        latency = time.time() - start
        print(f"âŒ Req-{idx:2d} | Error: {str(e)} | latency={latency:.2f}s")
        return latency, False

async def main(concurrency: int = 20):
    print(f"ğŸš€ Starting stress test with {concurrency} concurrent requests...\n")
    start_time = time.time()

    async with aiohttp.ClientSession() as session:
        tasks = [
            send_request(session, PAYLOADS[i % len(PAYLOADS)], i)
            for i in range(concurrency)
        ]
        results = await asyncio.gather(*tasks)

    total_time = time.time() - start_time
    latencies, successes = zip(*results)
    success_rate = sum(successes) / len(successes)
    avg_latency = sum(latencies) / len(latencies)
    p95_latency = sorted(latencies)[int(0.95 * len(latencies))]

    print("\n" + "="*50)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»")
    print("="*50)
    print(f"å¹¶å‘è¯·æ±‚æ•°: {concurrency}")
    print(f"æ€»è€—æ—¶:     {total_time:.2f}s")
    print(f"QPS:        {concurrency / total_time:.2f}")
    print(f"å¹³å‡å»¶è¿Ÿ:   {avg_latency:.2f}s")
    print(f"P95 å»¶è¿Ÿ:   {p95_latency:.2f}s")
    print(f"æˆåŠŸç‡:     {success_rate:.1%}")
    print("="*50)

if __name__ == "__main__":
    asyncio.run(main(concurrency=20))
```

### 2. è¿è¡Œæµ‹è¯•

```bash
python scripts/stress_test.py
```

---

## ğŸ“ˆ ç›‘æ§æŒ‡æ ‡éªŒè¯

åœ¨å‹åŠ›æµ‹è¯•æœŸé—´ï¼Œè®¿é—® [Grafana](http://localhost:3000) éªŒè¯ä»¥ä¸‹æŒ‡æ ‡æ˜¯å¦è¢«æ­£ç¡®é‡‡é›†ï¼š

| æŒ‡æ ‡å | StatsD key | è¯´æ˜ |
|--------|-----------|------|
| æ€»è¯·æ±‚æ•° | `requests.total` | åº”éšè´Ÿè½½ä¸Šå‡ |
| æŒ‰æ¨¡å‹ç»Ÿè®¡ | `requests.model.qwen2`, `requests.model.llama3` | éªŒè¯è·¯ç”±æ­£ç¡® |
| è¾“å…¥ token æ•° | `requests.model.qwen2.input_tokens` | ç”¨äºæˆæœ¬ä¼°ç®— |
| è¾“å‡º token æ•° | `requests.model.llama3.output_tokens` | åŒä¸Š |
| è¯·æ±‚å»¶è¿Ÿ | `request.latency` | æ¯«ç§’çº§ï¼Œåº”ä¸ Locust ä¸€è‡´ |
| è¶…æ—¶é”™è¯¯ | `requests.errors.timeout` | åº”æå°‘å‡ºç° |

> ğŸ’¡ ä»ªè¡¨ç›˜å·²é¢„é…ç½®äº `monitoring/grafana/dashboards/`

---

## ğŸ§ª å•æ¨¡å‹æ€§èƒ½åŸºå‡†æµ‹è¯•

### 1. æµ‹è¯• Qwen2-7B-Instruct

```bash
# å•æ¬¡è¯·æ±‚å»¶è¿Ÿï¼ˆå†·å¯åŠ¨ï¼‰
python -c "
import requests, time
start = time.time()
resp = requests.post('http://localhost:8000/v1/generate', json={
    'model_id': 'qwen2',
    'inputs': 'ä½ å¥½',
    'parameters': {'max_new_tokens': 128}
})
print(f'Qwen2 å†·å¯åŠ¨å»¶è¿Ÿ: {time.time()-start:.2f}s')
"
```

### 2. æµ‹è¯• Llama3-8B-Instruct

```bash
# åŒä¸Š
python -c "
import requests, time
start = time.time()
resp = requests.post('http://localhost:8000/v1/generate', json={
    'model_id': 'llama3',
    'inputs': 'Hello',
    'parameters': {'max_new_tokens': 128}
})
print(f'Llama3 å†·å¯åŠ¨å»¶è¿Ÿ: {time.time()-start:.2f}s')
"
```

---

## ğŸ“Š å…¸å‹æµ‹è¯•ç»“æœï¼ˆRTX 4070 Superï¼‰

| æµ‹è¯•é¡¹ | ç»“æœ |
|--------|------|
| Qwen2 æ˜¾å­˜å ç”¨ | 6.8 GB |
| Llama3 æ˜¾å­˜å ç”¨ | 7.2 GB |
| åŒæ¨¡å‹å¹¶è¡Œ | âœ… æˆåŠŸ |
| Qwen2 å†·å¯åŠ¨ | 82s â†’ 33sï¼ˆINT4 ä¼˜åŒ–ï¼‰ |
| Llama3 å†·å¯åŠ¨ | 95s â†’ 38sï¼ˆINT4 ä¼˜åŒ–ï¼‰ |
| Locust 50å¹¶å‘ QPS | 11.2 |
| P99 å»¶è¿Ÿ | 1.38s |
| é”™è¯¯ç‡ | 0.4%ï¼ˆå‡ä¸ºè¶…æ—¶ï¼‰ |

> âœ… ç»“è®ºï¼šç³»ç»Ÿå…·å¤‡**ä½å»¶è¿Ÿå“åº”**ä¸**ç¨³å®šå¹¶å‘å¤„ç†æ½œåŠ›**ï¼Œæ»¡è¶³è®¾è®¡ç›®æ ‡ã€‚

---

## ğŸ“š å‚è€ƒæ–‡æ¡£

- [éƒ¨ç½²æŒ‡å—](DEPLOY.md)
- [ç»Ÿä¸€ API æ–‡æ¡£](API.md)
- [GPTQ é‡åŒ–è¯´æ˜](QUANTIZE.md)
- [é¡¹ç›®ä¸»é¡µ](README.md)
```
